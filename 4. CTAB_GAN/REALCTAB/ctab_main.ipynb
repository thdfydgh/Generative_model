{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui-opksW3z24",
        "outputId": "dfc4cf71-4039-4b1c-f19b-f4a517fbbe5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.21.0\n",
            "  Downloading numpy-1.21.0.zip (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.1 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.9.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.21.0 torch==1.9.1 pandas==1.2.4 sklearn==0.24.1 dython==0.6.4.post1 scipy==1.4.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.mixture import BayesianGaussianMixture\n",
        "\n",
        "\n",
        "class DataTransformer():\n",
        "\n",
        "    \"\"\"\n",
        "    Transformer class responsible for processing data to train the CTABGANSynthesizer model\n",
        "\n",
        "    Variables:\n",
        "    1) train_data -> input dataframe\n",
        "    2) categorical_list -> list of categorical columns\n",
        "    3) mixed_dict -> dictionary of mixed columns\n",
        "    4) n_clusters -> number of modes to fit bayesian gaussian mixture (bgm) model\n",
        "    5) eps -> threshold for ignoring less prominent modes in the mixture model\n",
        "    6) ordering -> stores original ordering for modes of numeric columns\n",
        "    7) output_info -> stores dimension and output activations of columns (i.e., tanh for numeric, softmax for categorical)\n",
        "    8) output_dim -> stores the final column width of the transformed data\n",
        "    9) components -> stores the valid modes used by numeric columns\n",
        "    10) filter_arr -> stores valid indices of continuous component in mixed columns\n",
        "    11) meta -> stores column information corresponding to different data types i.e., categorical/mixed/numerical\n",
        "\n",
        "\n",
        "    Methods:\n",
        "    1) __init__() -> initializes transformer object and computes meta information of columns\n",
        "    2) get_metadata() -> builds an inventory of individual columns and stores their relevant properties\n",
        "    3) fit() -> fits the required bgm models to process the input data\n",
        "    4) transform() -> executes the transformation required to train the model\n",
        "    5) inverse_transform() -> executes the reverse transformation on data generated from the model\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_data=pd.DataFrame, categorical_list=[], mixed_dict={}, n_clusters=5, eps=0.005):\n",
        "\n",
        "        self.meta = None\n",
        "        self.train_data = train_data\n",
        "        self.categorical_columns= categorical_list\n",
        "        self.mixed_columns= mixed_dict\n",
        "        self.n_clusters = n_clusters\n",
        "        self.eps = eps\n",
        "        self.ordering = []\n",
        "        self.output_info = []\n",
        "        self.output_dim = 0\n",
        "        self.components = []\n",
        "        self.filter_arr = []\n",
        "        self.meta = self.get_metadata()\n",
        "\n",
        "    def get_metadata(self):\n",
        "\n",
        "        meta = []\n",
        "\n",
        "        for index in range(self.train_data.shape[1]):\n",
        "            column = self.train_data.iloc[:,index]\n",
        "            if index in self.categorical_columns:\n",
        "                mapper = column.value_counts().index.tolist()\n",
        "                meta.append({\n",
        "                        \"name\": index,\n",
        "                        \"type\": \"categorical\",\n",
        "                        \"size\": len(mapper),\n",
        "                        \"i2s\": mapper\n",
        "                })\n",
        "            elif index in self.mixed_columns.keys():\n",
        "                meta.append({\n",
        "                    \"name\": index,\n",
        "                    \"type\": \"mixed\",\n",
        "                    \"min\": column.min(),\n",
        "                    \"max\": column.max(),\n",
        "                    \"modal\": self.mixed_columns[index]\n",
        "                })\n",
        "            else:\n",
        "                meta.append({\n",
        "                    \"name\": index,\n",
        "                    \"type\": \"continuous\",\n",
        "                    \"min\": column.min(),\n",
        "                    \"max\": column.max(),\n",
        "                })\n",
        "\n",
        "        return meta\n",
        "\n",
        "    def fit(self):\n",
        "\n",
        "        data = self.train_data.values\n",
        "\n",
        "        # stores the corresponding bgm models for processing numeric data\n",
        "        model = []\n",
        "\n",
        "        # iterating through column information\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            if info['type'] == \"continuous\":\n",
        "                # fitting bgm model\n",
        "                gm = BayesianGaussianMixture(\n",
        "                    n_components=self.n_clusters,\n",
        "                    weight_concentration_prior_type='dirichlet_process',\n",
        "                    weight_concentration_prior=0.001, # lower values result in lesser modes being active\n",
        "                    max_iter=500, n_init=5, random_state=42, tol=1e-3)\n",
        "                gm.fit(data[:, id_].reshape([-1, 1]))\n",
        "                model.append(gm)\n",
        "                # keeping only relevant modes that have higher weight than eps and are used to fit the data\n",
        "                old_comp = gm.weights_ > self.eps\n",
        "                mode_freq = (pd.Series(gm.predict(data[:, id_].reshape([-1, 1]))).value_counts().keys())\n",
        "                comp = []\n",
        "                for i in range(self.n_clusters):\n",
        "                    if (i in (mode_freq)) & old_comp[i]:\n",
        "                        comp.append(True)\n",
        "                    else:\n",
        "                        comp.append(False)\n",
        "                self.components.append(comp)\n",
        "                self.output_info += [(1, 'tanh'), (np.sum(comp), 'softmax')]\n",
        "                self.output_dim += 1 + np.sum(comp)\n",
        "\n",
        "            elif info['type'] == \"mixed\":\n",
        "\n",
        "                # in case of mixed columns, two bgm models are used\n",
        "                gm1 = BayesianGaussianMixture(\n",
        "                    n_components=self.n_clusters,\n",
        "                    weight_concentration_prior_type='dirichlet_process',\n",
        "                    weight_concentration_prior=0.001, max_iter=500, n_init=5, random_state=42, tol=1e-3)\n",
        "                gm2 = BayesianGaussianMixture(\n",
        "                    n_components=self.n_clusters,\n",
        "                    weight_concentration_prior_type='dirichlet_process',\n",
        "                    weight_concentration_prior=0.001, max_iter=500, n_init=5, random_state=42, tol=1e-3)\n",
        "\n",
        "                # first bgm model is fit to the entire data only for the purposes of obtaining a normalized value of any particular categorical mode\n",
        "                gm1.fit(data[:, id_].reshape([-1, 1]))\n",
        "\n",
        "                # main bgm model used to fit the continuous component and serves the same purpose as with purely numeric columns\n",
        "                filter_arr = []\n",
        "                for element in data[:, id_]:\n",
        "                    if element not in info['modal']:\n",
        "                        filter_arr.append(True)\n",
        "                    else:\n",
        "                        filter_arr.append(False)\n",
        "                self.filter_arr.append(filter_arr)\n",
        "\n",
        "                gm2.fit(data[:, id_][filter_arr].reshape([-1, 1]))\n",
        "\n",
        "                model.append((gm1,gm2))\n",
        "\n",
        "                # similarly keeping only relevant modes with higher weight than eps and are used to fit strictly continuous data\n",
        "                old_comp = gm2.weights_ > self.eps\n",
        "                mode_freq = (pd.Series(gm2.predict(data[:, id_][filter_arr].reshape([-1, 1]))).value_counts().keys())\n",
        "                comp = []\n",
        "\n",
        "                for i in range(self.n_clusters):\n",
        "                    if (i in (mode_freq)) & old_comp[i]:\n",
        "                        comp.append(True)\n",
        "                    else:\n",
        "                        comp.append(False)\n",
        "\n",
        "                self.components.append(comp)\n",
        "\n",
        "                # modes of the categorical component are appended to modes produced by the main bgm model\n",
        "                self.output_info += [(1, 'tanh'), (np.sum(comp) + len(info['modal']), 'softmax')]\n",
        "                self.output_dim += 1 + np.sum(comp) + len(info['modal'])\n",
        "\n",
        "            else:\n",
        "                # in case of categorical columns, bgm model is ignored\n",
        "                model.append(None)\n",
        "                self.components.append(None)\n",
        "                self.output_info += [(info['size'], 'softmax')]\n",
        "                self.output_dim += info['size']\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def transform(self, data):\n",
        "\n",
        "        # stores the transformed values\n",
        "        values = []\n",
        "\n",
        "        # used for accessing filter_arr for transforming mixed columns\n",
        "        mixed_counter = 0\n",
        "\n",
        "        # iterating through column information\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            current = data[:, id_]\n",
        "            if info['type'] == \"continuous\":\n",
        "                # mode-specific normalization occurs here\n",
        "                current = current.reshape([-1, 1])\n",
        "                # means and stds of the modes are obtained from the corresponding fitted bgm model\n",
        "                means = self.model[id_].means_.reshape((1, self.n_clusters))\n",
        "                stds = np.sqrt(self.model[id_].covariances_).reshape((1, self.n_clusters))\n",
        "                # values are then normalized and stored for all modes\n",
        "                features = np.empty(shape=(len(current),self.n_clusters))\n",
        "                # note 4 is a multiplier to ensure values lie between -1 to 1 but this is not always guaranteed\n",
        "                features = (current - means) / (4 * stds)\n",
        "\n",
        "                # number of distict modes\n",
        "                n_opts = sum(self.components[id_])\n",
        "                # storing the mode for each data point by sampling from the probability mass distribution across all modes based on fitted bgm model\n",
        "                opt_sel = np.zeros(len(data), dtype='int')\n",
        "                probs = self.model[id_].predict_proba(current.reshape([-1, 1]))\n",
        "                probs = probs[:, self.components[id_]]\n",
        "                for i in range(len(data)):\n",
        "                    pp = probs[i] + 1e-6\n",
        "                    pp = pp / sum(pp)\n",
        "                    opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n",
        "\n",
        "                # creating a one-hot-encoding for the corresponding selected modes\n",
        "                probs_onehot = np.zeros_like(probs)\n",
        "                probs_onehot[np.arange(len(probs)), opt_sel] = 1\n",
        "\n",
        "                # obtaining the normalized values based on the appropriately selected mode and clipping to ensure values are within (-1,1)\n",
        "                idx = np.arange((len(features)))\n",
        "                features = features[:, self.components[id_]]\n",
        "                features = features[idx, opt_sel].reshape([-1, 1])\n",
        "                features = np.clip(features, -.99, .99)\n",
        "\n",
        "                # re-ordering the one-hot-encoding of modes in descending order as per their frequency of being selected\n",
        "                re_ordered_phot = np.zeros_like(probs_onehot)\n",
        "                col_sums = probs_onehot.sum(axis=0)\n",
        "                n = probs_onehot.shape[1]\n",
        "                largest_indices = np.argsort(-1*col_sums)[:n]\n",
        "                for id,val in enumerate(largest_indices):\n",
        "                    re_ordered_phot[:,id] = probs_onehot[:,val]\n",
        "\n",
        "                # storing the original ordering for invoking inverse transform\n",
        "                self.ordering.append(largest_indices)\n",
        "\n",
        "                # storing transformed numeric column represented as normalized values and corresponding modes\n",
        "                values += [features, re_ordered_phot]\n",
        "\n",
        "            elif info['type'] == \"mixed\":\n",
        "\n",
        "                # means and standard deviation of modes obtained from the first fitted bgm model\n",
        "                means_0 = self.model[id_][0].means_.reshape([-1])\n",
        "                stds_0 = np.sqrt(self.model[id_][0].covariances_).reshape([-1])\n",
        "\n",
        "                # list to store relevant bgm modes for categorical components\n",
        "                zero_std_list = []\n",
        "\n",
        "                # means and stds needed to normalize relevant categorical components\n",
        "                means_needed = []\n",
        "                stds_needed = []\n",
        "\n",
        "                # obtaining the closest bgm mode to the categorical component\n",
        "                for mode in info['modal']:\n",
        "                    # skipped for mode representing missing values\n",
        "                    if mode!=-9999999:\n",
        "                        dist = []\n",
        "                        for idx,val in enumerate(list(means_0.flatten())):\n",
        "                            dist.append(abs(mode-val))\n",
        "                        index_min = np.argmin(np.array(dist))\n",
        "                        zero_std_list.append(index_min)\n",
        "                    else: continue\n",
        "\n",
        "\n",
        "                # stores the appropriate normalized value of categorical modes\n",
        "                mode_vals = []\n",
        "\n",
        "                # based on the means and stds of the chosen modes for categorical components, their respective values are similarly normalized\n",
        "                for idx in zero_std_list:\n",
        "                    means_needed.append(means_0[idx])\n",
        "                    stds_needed.append(stds_0[idx])\n",
        "\n",
        "                for i,j,k in zip(info['modal'],means_needed,stds_needed):\n",
        "                    this_val  = np.clip(((i - j) / (4*k)), -.99, .99)\n",
        "                    mode_vals.append(this_val)\n",
        "\n",
        "                # for categorical modes representing missing values, the normalized value associated is simply 0\n",
        "                if -9999999 in info[\"modal\"]:\n",
        "                    mode_vals.append(0)\n",
        "\n",
        "                # transforming continuous component of mixed columns similar to purely numeric columns using second fitted bgm model\n",
        "                current = current.reshape([-1, 1])\n",
        "                filter_arr = self.filter_arr[mixed_counter]\n",
        "                current = current[filter_arr]\n",
        "\n",
        "                means = self.model[id_][1].means_.reshape((1, self.n_clusters))\n",
        "                stds = np.sqrt(self.model[id_][1].covariances_).reshape((1, self.n_clusters))\n",
        "\n",
        "                features = np.empty(shape=(len(current),self.n_clusters))\n",
        "                features = (current - means) / (4 * stds)\n",
        "\n",
        "                n_opts = sum(self.components[id_])\n",
        "                probs = self.model[id_][1].predict_proba(current.reshape([-1, 1]))\n",
        "                probs = probs[:, self.components[id_]]\n",
        "\n",
        "                opt_sel = np.zeros(len(current), dtype='int')\n",
        "                for i in range(len(current)):\n",
        "                    pp = probs[i] + 1e-6\n",
        "                    pp = pp / sum(pp)\n",
        "                    opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n",
        "\n",
        "                idx = np.arange((len(features)))\n",
        "                features = features[:, self.components[id_]]\n",
        "                features = features[idx, opt_sel].reshape([-1, 1])\n",
        "                features = np.clip(features, -.99, .99)\n",
        "\n",
        "                probs_onehot = np.zeros_like(probs)\n",
        "                probs_onehot[np.arange(len(probs)), opt_sel] = 1\n",
        "\n",
        "                # additional modes are appended to represent categorical component\n",
        "                extra_bits = np.zeros([len(current), len(info['modal'])])\n",
        "                temp_probs_onehot = np.concatenate([extra_bits,probs_onehot], axis = 1)\n",
        "\n",
        "                # storing the final normalized value and one-hot-encoding of selected modes\n",
        "                final = np.zeros([len(data), 1 + probs_onehot.shape[1] + len(info['modal'])])\n",
        "\n",
        "                # iterates through only the continuous component\n",
        "                features_curser = 0\n",
        "\n",
        "                for idx, val in enumerate(data[:, id_]):\n",
        "\n",
        "                    if val in info['modal']:\n",
        "                        # dealing with the modes of categorical component\n",
        "                        category_ = list(map(info['modal'].index, [val]))[0]\n",
        "                        final[idx, 0] = mode_vals[category_]\n",
        "                        final[idx, (category_+1)] = 1\n",
        "\n",
        "                    else:\n",
        "                        # dealing with the modes of continuous component\n",
        "                        final[idx, 0] = features[features_curser]\n",
        "                        final[idx, (1+len(info['modal'])):] = temp_probs_onehot[features_curser][len(info['modal']):]\n",
        "                        features_curser = features_curser + 1\n",
        "\n",
        "                # re-ordering the one-hot-encoding of modes in descending order as per their frequency of being selected\n",
        "                just_onehot = final[:,1:]\n",
        "                re_ordered_jhot= np.zeros_like(just_onehot)\n",
        "                n = just_onehot.shape[1]\n",
        "                col_sums = just_onehot.sum(axis=0)\n",
        "                largest_indices = np.argsort(-1*col_sums)[:n]\n",
        "\n",
        "                for id,val in enumerate(largest_indices):\n",
        "                      re_ordered_jhot[:,id] = just_onehot[:,val]\n",
        "\n",
        "                final_features = final[:,0].reshape([-1, 1])\n",
        "\n",
        "                # storing the original ordering for invoking inverse transform\n",
        "                self.ordering.append(largest_indices)\n",
        "\n",
        "                values += [final_features, re_ordered_jhot]\n",
        "\n",
        "                mixed_counter = mixed_counter + 1\n",
        "\n",
        "            else:\n",
        "                # for categorical columns, standard one-hot-encoding is applied where categories are in descending order of frequency by default\n",
        "                self.ordering.append(None)\n",
        "                col_t = np.zeros([len(data), info['size']])\n",
        "                idx = list(map(info['i2s'].index, current))\n",
        "                col_t[np.arange(len(data)), idx] = 1\n",
        "                values.append(col_t)\n",
        "\n",
        "        return np.concatenate(values, axis=1)\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "\n",
        "        # stores the final inverse transformed generated data\n",
        "        data_t = np.zeros([len(data), len(self.meta)])\n",
        "\n",
        "        # used to iterate through the columns of the raw generated data\n",
        "        st = 0\n",
        "\n",
        "        # iterating through original column information\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            if info['type'] == \"continuous\":\n",
        "\n",
        "                # obtaining the generated normalized values and clipping for stability\n",
        "                u = data[:, st]\n",
        "                u = np.clip(u, -1, 1)\n",
        "\n",
        "                # obtaining the one-hot-encoding of the modes representing the normalized values\n",
        "                v = data[:, st + 1:st + 1 + np.sum(self.components[id_])]\n",
        "\n",
        "                # re-ordering the modes as per their original ordering\n",
        "                order = self.ordering[id_]\n",
        "                v_re_ordered = np.zeros_like(v)\n",
        "                for id,val in enumerate(order):\n",
        "                    v_re_ordered[:,val] = v[:,id]\n",
        "                v = v_re_ordered\n",
        "\n",
        "                # ensuring un-used modes are represented with -100 such that they can be ignored when computing argmax\n",
        "                v_t = np.ones((data.shape[0], self.n_clusters)) * -100\n",
        "                v_t[:, self.components[id_]] = v\n",
        "                v = v_t\n",
        "\n",
        "                # obtaining approriate means and stds as per the appropriately selected mode for each data point based on fitted bgm model\n",
        "                means = self.model[id_].means_.reshape([-1])\n",
        "                stds = np.sqrt(self.model[id_].covariances_).reshape([-1])\n",
        "                p_argmax = np.argmax(v, axis=1)\n",
        "                std_t = stds[p_argmax]\n",
        "                mean_t = means[p_argmax]\n",
        "\n",
        "                # executing the inverse transformation\n",
        "                tmp = u * 4 * std_t + mean_t\n",
        "\n",
        "                data_t[:, id_] = tmp\n",
        "\n",
        "                # moving to the next set of columns in the raw generated data in correspondance to original column information\n",
        "                st += 1 + np.sum(self.components[id_])\n",
        "\n",
        "            elif info['type'] == \"mixed\":\n",
        "\n",
        "                # obtaining the generated normalized values and corresponding modes\n",
        "                u = data[:, st]\n",
        "                u = np.clip(u, -1, 1)\n",
        "                full_v = data[:,(st+1):(st+1)+len(info['modal'])+np.sum(self.components[id_])]\n",
        "\n",
        "                # re-ordering the modes as per their original ordering\n",
        "                order = self.ordering[id_]\n",
        "                full_v_re_ordered = np.zeros_like(full_v)\n",
        "                for id,val in enumerate(order):\n",
        "                    full_v_re_ordered[:,val] = full_v[:,id]\n",
        "                full_v = full_v_re_ordered\n",
        "\n",
        "                # modes of categorical component\n",
        "                mixed_v = full_v[:,:len(info['modal'])]\n",
        "\n",
        "                # modes of continuous component\n",
        "                v = full_v[:,-np.sum(self.components[id_]):]\n",
        "\n",
        "                # similarly ensuring un-used modes are represented with -100 to be ignored while computing argmax\n",
        "                v_t = np.ones((data.shape[0], self.n_clusters)) * -100\n",
        "                v_t[:, self.components[id_]] = v\n",
        "                v = np.concatenate([mixed_v,v_t], axis=1)\n",
        "                p_argmax = np.argmax(v, axis=1)\n",
        "\n",
        "                # obtaining the means and stds of the continuous component using second fitted bgm model\n",
        "                means = self.model[id_][1].means_.reshape([-1])\n",
        "                stds = np.sqrt(self.model[id_][1].covariances_).reshape([-1])\n",
        "\n",
        "                # used to store the inverse-transformed data points\n",
        "                result = np.zeros_like(u)\n",
        "\n",
        "                for idx in range(len(data)):\n",
        "                    # in case of categorical mode being selected, the mode value itself is simply assigned\n",
        "                    if p_argmax[idx] < len(info['modal']):\n",
        "                        argmax_value = p_argmax[idx]\n",
        "                        result[idx] = float(list(map(info['modal'].__getitem__, [argmax_value]))[0])\n",
        "                    else:\n",
        "                        # in case of continuous mode being selected, similar inverse-transform for purely numeric values is applied\n",
        "                        std_t = stds[(p_argmax[idx]-len(info['modal']))]\n",
        "                        mean_t = means[(p_argmax[idx]-len(info['modal']))]\n",
        "                        result[idx] = u[idx] * 4 * std_t + mean_t\n",
        "\n",
        "                data_t[:, id_] = result\n",
        "\n",
        "                st += 1 + np.sum(self.components[id_]) + len(info['modal'])\n",
        "\n",
        "            else:\n",
        "                # reversing one hot encoding back to label encoding for categorical columns\n",
        "                current = data[:, st:st + info['size']]\n",
        "                idx = np.argmax(current, axis=1)\n",
        "                data_t[:, id_] = list(map(info['i2s'].__getitem__, idx))\n",
        "                st += info['size']\n",
        "        return data_t\n",
        "\n",
        "class ImageTransformer():\n",
        "\n",
        "    \"\"\"\n",
        "    Transformer responsible for translating data rows to images and vice versa\n",
        "\n",
        "    Variables:\n",
        "    1) side -> height/width of the image\n",
        "\n",
        "    Methods:\n",
        "    1) __init__() -> initializes image transformer object with given input\n",
        "    2) transform() -> converts tabular data records into square image format\n",
        "    3) inverse_transform() -> converts square images into tabular format\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, side):\n",
        "\n",
        "        self.height = side\n",
        "\n",
        "    def transform(self, data):\n",
        "\n",
        "        if self.height * self.height > len(data[0]):\n",
        "            # tabular data records are padded with 0 to conform to square shaped images\n",
        "            padding = torch.zeros((len(data), self.height * self.height - len(data[0]))).to(data.device)\n",
        "            data = torch.cat([data, padding], axis=1)\n",
        "\n",
        "        return data.view(-1, 1, self.height, self.height)\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "\n",
        "        data = data.view(-1, self.height * self.height)\n",
        "\n",
        "        return data\n",
        "\n"
      ],
      "metadata": {
        "id": "hz2nAJk84OVU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import (Dropout, LeakyReLU, Linear, Module, ReLU, Sequential,\n",
        "Conv2d, ConvTranspose2d, BatchNorm2d, Sigmoid, init, BCELoss, CrossEntropyLoss,SmoothL1Loss)\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def random_choice_prob_index_sampling(probs,col_idx):\n",
        "\n",
        "    \"\"\"\n",
        "    Used to sample a specific category within a chosen one-hot-encoding representation\n",
        "\n",
        "    Inputs:\n",
        "    1) probs -> probability mass distribution of categories\n",
        "    2) col_idx -> index used to identify any given one-hot-encoding\n",
        "\n",
        "    Outputs:\n",
        "    1) option_list -> list of chosen categories\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    option_list = []\n",
        "    for i in col_idx:\n",
        "        # for improved stability\n",
        "        pp = probs[i] + 1e-6\n",
        "        pp = pp / sum(pp)\n",
        "        # sampled based on given probability mass distribution of categories within the given one-hot-encoding\n",
        "        option_list.append(np.random.choice(np.arange(len(probs[i])), p=pp))\n",
        "\n",
        "    return np.array(option_list).reshape(col_idx.shape)\n",
        "\n",
        "class Condvec(object):\n",
        "\n",
        "    \"\"\"\n",
        "    This class is responsible for sampling conditional vectors to be supplied to the generator\n",
        "\n",
        "    Variables:\n",
        "    1) model -> list containing an index of highlighted categories in their corresponding one-hot-encoded represenations\n",
        "    2) interval -> an array holding the respective one-hot-encoding starting positions and sizes\n",
        "    3) n_col -> total no. of one-hot-encoding representations\n",
        "    4) n_opt -> total no. of distinct categories across all one-hot-encoding representations\n",
        "    5) p_log_sampling -> list containing log of probability mass distribution of categories within their respective one-hot-encoding representations\n",
        "    6) p_sampling -> list containing probability mass distribution of categories within their respective one-hot-encoding representations\n",
        "\n",
        "    Methods:\n",
        "    1) __init__() -> takes transformed input data with respective column information to compute class variables\n",
        "    2) sample_train() -> used to sample the conditional vector during training of the model\n",
        "    3) sample() -> used to sample the conditional vector for generating data after training is finished\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, data, output_info):\n",
        "\n",
        "        self.model = []\n",
        "        self.interval = []\n",
        "        self.n_col = 0\n",
        "        self.n_opt = 0\n",
        "        self.p_log_sampling = []\n",
        "        self.p_sampling = []\n",
        "\n",
        "        # iterating through the transformed input data columns\n",
        "        st = 0\n",
        "        for item in output_info:\n",
        "            # ignoring columns that do not represent one-hot-encodings\n",
        "            if item[1] == 'tanh':\n",
        "                st += item[0]\n",
        "                continue\n",
        "            elif item[1] == 'softmax':\n",
        "                # using starting (st) and ending (ed) position of any given one-hot-encoded representation to obtain relevant information\n",
        "                ed = st + item[0]\n",
        "                self.model.append(np.argmax(data[:, st:ed], axis=-1))\n",
        "                self.interval.append((self.n_opt, item[0]))\n",
        "                self.n_col += 1\n",
        "                self.n_opt += item[0]\n",
        "                freq = np.sum(data[:, st:ed], axis=0)\n",
        "                log_freq = np.log(freq + 1)\n",
        "                log_pmf = log_freq / np.sum(log_freq)\n",
        "                self.p_log_sampling.append(log_pmf)\n",
        "                pmf = freq / np.sum(freq)\n",
        "                self.p_sampling.append(pmf)\n",
        "                st = ed\n",
        "\n",
        "        self.interval = np.asarray(self.interval)\n",
        "\n",
        "    def sample_train(self, batch):\n",
        "\n",
        "        \"\"\"\n",
        "        Used to create the conditional vectors for feeding it to the generator during training\n",
        "\n",
        "        Inputs:\n",
        "        1) batch -> no. of data records to be generated in a batch\n",
        "\n",
        "        Outputs:\n",
        "        1) vec -> a matrix containing a conditional vector for each data point to be generated\n",
        "        2) mask -> a matrix to identify chosen one-hot-encodings across the batch\n",
        "        3) idx -> list of chosen one-hot encoding across the batch\n",
        "        4) opt1prime -> selected categories within chosen one-hot-encodings\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if self.n_col == 0:\n",
        "            return None\n",
        "        batch = batch\n",
        "\n",
        "        # each conditional vector in vec is a one-hot vector used to highlight a specific category across all possible one-hot-encoded representations\n",
        "        # (i.e., including modes of continuous and mixed columns)\n",
        "        vec = np.zeros((batch, self.n_opt), dtype='float32')\n",
        "\n",
        "        # choosing one specific one-hot-encoding from all possible one-hot-encoded representations\n",
        "        idx = np.random.choice(np.arange(self.n_col), batch)\n",
        "\n",
        "        # matrix of shape (batch x total no. of one-hot-encoded representations) with 1 in indexes of chosen representations and 0 elsewhere\n",
        "        mask = np.zeros((batch, self.n_col), dtype='float32')\n",
        "        mask[np.arange(batch), idx] = 1\n",
        "\n",
        "        # producing a list of selected categories within each of selected one-hot-encoding representation\n",
        "        opt1prime = random_choice_prob_index_sampling(self.p_log_sampling,idx)\n",
        "\n",
        "        # assigning the appropriately chosen category for each corresponding conditional vector\n",
        "        for i in np.arange(batch):\n",
        "            vec[i, self.interval[idx[i], 0] + opt1prime[i]] = 1\n",
        "\n",
        "        return vec, mask, idx, opt1prime\n",
        "\n",
        "    def sample(self, batch):\n",
        "\n",
        "        \"\"\"\n",
        "        Used to create the conditional vectors for feeding it to the generator after training is finished\n",
        "\n",
        "        Inputs:\n",
        "        1) batch -> no. of data records to be generated in a batch\n",
        "\n",
        "        Outputs:\n",
        "        1) vec -> an array containing a conditional vector for each data point to be generated\n",
        "        \"\"\"\n",
        "\n",
        "        if self.n_col == 0:\n",
        "            return None\n",
        "\n",
        "        batch = batch\n",
        "\n",
        "        # each conditional vector in vec is a one-hot vector used to highlight a specific category across all possible one-hot-encoded representations\n",
        "        # (i.e., including modes of continuous and mixed columns)\n",
        "        vec = np.zeros((batch, self.n_opt), dtype='float32')\n",
        "\n",
        "        # choosing one specific one-hot-encoding from all possible one-hot-encoded representations\n",
        "        idx = np.random.choice(np.arange(self.n_col), batch)\n",
        "\n",
        "        # producing a list of selected categories within each of selected one-hot-encoding representation\n",
        "        opt1prime = random_choice_prob_index_sampling(self.p_sampling,idx)\n",
        "\n",
        "        # assigning the appropriately chosen category for each corresponding conditional vector\n",
        "        for i in np.arange(batch):\n",
        "            vec[i, self.interval[idx[i], 0] + opt1prime[i]] = 1\n",
        "\n",
        "        return vec\n",
        "\n",
        "def cond_loss(data, output_info, c, m):\n",
        "\n",
        "    \"\"\"\n",
        "    Used to compute the conditional loss for ensuring the generator produces the desired category as specified by the conditional vector\n",
        "\n",
        "    Inputs:\n",
        "    1) data -> raw data synthesized by the generator\n",
        "    2) output_info -> column informtion corresponding to the data transformer\n",
        "    3) c -> conditional vectors used to synthesize a batch of data\n",
        "    4) m -> a matrix to identify chosen one-hot-encodings across the batch\n",
        "\n",
        "    Outputs:\n",
        "    1) loss -> conditional loss corresponding to the generated batch\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # used to store cross entropy loss between conditional vector and all generated one-hot-encodings\n",
        "    tmp_loss = []\n",
        "    # counter to iterate generated data columns\n",
        "    st = 0\n",
        "    # counter to iterate conditional vector\n",
        "    st_c = 0\n",
        "    # iterating through column information\n",
        "    for item in output_info:\n",
        "        # ignoring numeric columns\n",
        "        if item[1] == 'tanh':\n",
        "            st += item[0]\n",
        "            continue\n",
        "        # computing cross entropy loss between generated one-hot-encoding and corresponding encoding of conditional vector\n",
        "        elif item[1] == 'softmax':\n",
        "            ed = st + item[0]\n",
        "            ed_c = st_c + item[0]\n",
        "            tmp = F.cross_entropy(\n",
        "            data[:, st:ed],\n",
        "            torch.argmax(c[:, st_c:ed_c], dim=1),\n",
        "            reduction='none')\n",
        "            tmp_loss.append(tmp)\n",
        "            st = ed\n",
        "            st_c = ed_c\n",
        "\n",
        "    # computing the loss across the batch only and only for the relevant one-hot-encodings by applying the mask\n",
        "    tmp_loss = torch.stack(tmp_loss, dim=1)\n",
        "    loss = (tmp_loss * m).sum() / data.size()[0]\n",
        "\n",
        "    return loss\n",
        "\n",
        "class Sampler(object):\n",
        "\n",
        "    \"\"\"\n",
        "    This class is used to sample the transformed real data according to the conditional vector\n",
        "\n",
        "    Variables:\n",
        "    1) data -> real transformed input data\n",
        "    2) model -> stores the index values of data records corresponding to any given selected categories for all columns\n",
        "    3) n -> size of the input data\n",
        "\n",
        "    Methods:\n",
        "    1) __init__() -> initiates the sampler object and stores class variables\n",
        "    2) sample() -> takes as input the number of rows to be sampled (n), chosen column (col)\n",
        "                   and category within the column (opt) to sample real records accordingly\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, output_info):\n",
        "\n",
        "        super(Sampler, self).__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.model = []\n",
        "        self.n = len(data)\n",
        "\n",
        "        # counter to iterate through columns\n",
        "        st = 0\n",
        "        # iterating through column information\n",
        "        for item in output_info:\n",
        "            # ignoring numeric columns\n",
        "            if item[1] == 'tanh':\n",
        "                st += item[0]\n",
        "                continue\n",
        "            # storing indices of data records for all categories within one-hot-encoded representations\n",
        "            elif item[1] == 'softmax':\n",
        "                ed = st + item[0]\n",
        "                tmp = []\n",
        "                # iterating through each category within a one-hot-encoding\n",
        "                for j in range(item[0]):\n",
        "                    # storing the relevant indices of data records for the given categories\n",
        "                    tmp.append(np.nonzero(data[:, st + j])[0])\n",
        "                self.model.append(tmp)\n",
        "                st = ed\n",
        "\n",
        "    def sample(self, n, col, opt):\n",
        "\n",
        "        # if there are no one-hot-encoded representations, we may ignore sampling using a conditional vector\n",
        "        if col is None:\n",
        "            idx = np.random.choice(np.arange(self.n), n)\n",
        "            return self.data[idx]\n",
        "\n",
        "        # used to store relevant indices of data records based on selected category within a chosen one-hot-encoding\n",
        "        idx = []\n",
        "\n",
        "        # sampling a data record index randomly from all possible indices that meet the given criteria of the chosen category and one-hot-encoding\n",
        "        for c, o in zip(col, opt):\n",
        "            idx.append(np.random.choice(self.model[c][o]))\n",
        "\n",
        "        return self.data[idx]\n",
        "\n",
        "def get_st_ed(target_col_index,output_info):\n",
        "\n",
        "    \"\"\"\n",
        "    Used to obtain the start and ending positions of the target column as per the transformed data to be used by the classifier\n",
        "\n",
        "    Inputs:\n",
        "    1) target_col_index -> column index of the target column used for machine learning tasks (binary/multi-classification) in the raw data\n",
        "    2) output_info -> column information corresponding to the data after applying the data transformer\n",
        "\n",
        "    Outputs:\n",
        "    1) starting (st) and ending (ed) positions of the target column as per the transformed data\n",
        "\n",
        "    \"\"\"\n",
        "    # counter to iterate through columns\n",
        "    st = 0\n",
        "    # counter to check if the target column index has been reached\n",
        "    c= 0\n",
        "    # counter to iterate through column information\n",
        "    tc= 0\n",
        "    # iterating until target index has reached to obtain starting position of the one-hot-encoding used to represent target column in transformed data\n",
        "    for item in output_info:\n",
        "        # exiting loop if target index has reached\n",
        "        if c==target_col_index:\n",
        "            break\n",
        "        if item[1]=='tanh':\n",
        "            st += item[0]\n",
        "        elif item[1] == 'softmax':\n",
        "            st += item[0]\n",
        "            c+=1\n",
        "        tc+=1\n",
        "\n",
        "    # obtaining the ending position by using the dimension size of the one-hot-encoding used to represent the target column\n",
        "    ed= st+output_info[tc][0]\n",
        "\n",
        "    return (st,ed)\n",
        "\n",
        "class Classifier(Module):\n",
        "\n",
        "    \"\"\"\n",
        "    This class represents the classifier module used along side the discriminator to train the generator network\n",
        "\n",
        "    Variables:\n",
        "    1) dim -> column dimensionality of the transformed input data after removing target column\n",
        "    2) class_dims -> list of dimensions used for the hidden layers of the classifier network\n",
        "    3) str_end -> tuple containing the starting and ending positions of the target column in the transformed input data\n",
        "\n",
        "    Methods:\n",
        "    1) __init__() -> initializes and builds the layers of the classifier module\n",
        "    2) forward() -> executes the forward pass of the classifier module on the corresponding input data and\n",
        "                    outputs the predictions and corresponding true labels for the target column\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,input_dim, class_dims,st_ed):\n",
        "        super(Classifier,self).__init__()\n",
        "        # subtracting the target column size from the input dimensionality\n",
        "        self.dim = input_dim-(st_ed[1]-st_ed[0])\n",
        "        # storing the starting and ending positons of the target column in the input data\n",
        "        self.str_end = st_ed\n",
        "\n",
        "        # building the layers of the network with same hidden layers as discriminator\n",
        "        seq = []\n",
        "        tmp_dim = self.dim\n",
        "        for item in list(class_dims):\n",
        "            seq += [\n",
        "                Linear(tmp_dim, item),\n",
        "                LeakyReLU(0.2),\n",
        "                Dropout(0.5)\n",
        "            ]\n",
        "            tmp_dim = item\n",
        "\n",
        "        # in case of binary classification the last layer outputs a single numeric value which is squashed to a probability with sigmoid\n",
        "        if (st_ed[1]-st_ed[0])==2:\n",
        "            seq += [Linear(tmp_dim, 1),Sigmoid()]\n",
        "        # in case of multi-classs classification, the last layer outputs an array of numeric values associated to each class\n",
        "        else: seq += [Linear(tmp_dim,(st_ed[1]-st_ed[0]))]\n",
        "\n",
        "        self.seq = Sequential(*seq)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        # true labels obtained from the input data\n",
        "        label = torch.argmax(input[:, self.str_end[0]:self.str_end[1]], axis=-1)\n",
        "\n",
        "        # input to be fed to the classifier module\n",
        "        new_imp = torch.cat((input[:,:self.str_end[0]],input[:,self.str_end[1]:]),1)\n",
        "\n",
        "        # returning predictions and true labels for binary/multi-class classification\n",
        "        if ((self.str_end[1]-self.str_end[0])==2):\n",
        "            return self.seq(new_imp).view(-1), label\n",
        "        else: return self.seq(new_imp), label\n",
        "\n",
        "class Discriminator(Module):\n",
        "\n",
        "    \"\"\"\n",
        "    This class represents the discriminator network of the model\n",
        "\n",
        "    Variables:\n",
        "    1) seq -> layers of the network used for making the final prediction of the discriminator model\n",
        "    2) seq_info -> layers of the discriminator network used for computing the information loss\n",
        "\n",
        "    Methods:\n",
        "    1) __init__() -> initializes and builds the layers of the discriminator model\n",
        "    2) forward() -> executes a forward pass on the input data to output the final predictions and corresponding\n",
        "                    feature information associated with the penultimate layer used to compute the information loss\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.seq = Sequential(*layers)\n",
        "        self.seq_info = Sequential(*layers[:len(layers)-2])\n",
        "\n",
        "    def forward(self, input):\n",
        "        return (self.seq(input)), self.seq_info(input)\n",
        "\n",
        "class Generator(Module):\n",
        "\n",
        "    \"\"\"\n",
        "    This class represents the discriminator network of the model\n",
        "\n",
        "    Variables:\n",
        "    1) seq -> layers of the network used by the generator\n",
        "\n",
        "    Methods:\n",
        "    1) __init__() -> initializes and builds the layers of the generator model\n",
        "    2) forward() -> executes a forward pass using noise as input to generate data\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers):\n",
        "        super(Generator, self).__init__()\n",
        "        self.seq = Sequential(*layers)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.seq(input)\n",
        "\n",
        "def determine_layers_disc(side, num_channels):\n",
        "\n",
        "    \"\"\"\n",
        "    This function describes the layers of the discriminator network as per DCGAN (https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n",
        "\n",
        "    Inputs:\n",
        "    1) side -> height/width of the input fed to the discriminator\n",
        "    2) num_channels -> no. of channels used to decide the size of respective hidden layers\n",
        "\n",
        "    Outputs:\n",
        "    1) layers_D -> layers of the discriminator network\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # computing the dimensionality of hidden layers\n",
        "    layer_dims = [(1, side), (num_channels, side // 2)]\n",
        "\n",
        "    while layer_dims[-1][1] > 3 and len(layer_dims) < 4:\n",
        "        # the number of channels increases by a factor of 2 whereas the height/width decreases by the same factor with each layer\n",
        "        layer_dims.append((layer_dims[-1][0] * 2, layer_dims[-1][1] // 2))\n",
        "\n",
        "    # constructing the layers of the discriminator network based on the recommendations mentioned in https://arxiv.org/abs/1511.06434\n",
        "    layers_D = []\n",
        "    for prev, curr in zip(layer_dims, layer_dims[1:]):\n",
        "        layers_D += [\n",
        "            Conv2d(prev[0], curr[0], 4, 2, 1, bias=False),\n",
        "            BatchNorm2d(curr[0]),\n",
        "            LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "    # last layer reduces the output to a single numeric value which is squashed to a probabability using sigmoid function\n",
        "    layers_D += [\n",
        "        Conv2d(layer_dims[-1][0], 1, layer_dims[-1][1], 1, 0),\n",
        "        Sigmoid()\n",
        "    ]\n",
        "\n",
        "    return layers_D\n",
        "\n",
        "def determine_layers_gen(side, random_dim, num_channels):\n",
        "\n",
        "    \"\"\"\n",
        "    This function describes the layers of the generator network\n",
        "\n",
        "    Inputs:\n",
        "    1) random_dim -> height/width of the noise matrix to be fed for generation\n",
        "    2) num_channels -> no. of channels used to decide the size of respective hidden layers\n",
        "\n",
        "    Outputs:\n",
        "    1) layers_G -> layers of the generator network\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # computing the dimensionality of hidden layers\n",
        "    layer_dims = [(1, side), (num_channels, side // 2)]\n",
        "\n",
        "    while layer_dims[-1][1] > 3 and len(layer_dims) < 4:\n",
        "        layer_dims.append((layer_dims[-1][0] * 2, layer_dims[-1][1] // 2))\n",
        "\n",
        "    # similarly constructing the layers of the generator network based on the recommendations mentioned in https://arxiv.org/abs/1511.06434\n",
        "    # first layer of the generator takes the channel dimension of the noise matrix to the desired maximum channel size of the generator's layers\n",
        "    layers_G = [\n",
        "        ConvTranspose2d(\n",
        "            random_dim, layer_dims[-1][0], layer_dims[-1][1], 1, 0, output_padding=0, bias=False)\n",
        "    ]\n",
        "\n",
        "    # the following layers are then reversed with respect to the discriminator\n",
        "    # such as the no. of channels reduce by a factor of 2 and height/width of generated image increases by the same factor with each layer\n",
        "    for prev, curr in zip(reversed(layer_dims), reversed(layer_dims[:-1])):\n",
        "        layers_G += [\n",
        "            BatchNorm2d(prev[0]),\n",
        "            ReLU(True),\n",
        "            ConvTranspose2d(prev[0], curr[0], 4, 2, 1, output_padding=0, bias=True)\n",
        "        ]\n",
        "\n",
        "    return layers_G\n",
        "\n",
        "def apply_activate(data, output_info):\n",
        "\n",
        "    \"\"\"\n",
        "    This function applies the final activation corresponding to the column information associated with transformer\n",
        "\n",
        "    Inputs:\n",
        "    1) data -> input data generated by the model in the same format as the transformed input data\n",
        "    2) output_info -> column information associated with the transformed input data\n",
        "\n",
        "    Outputs:\n",
        "    1) act_data -> resulting data after applying the respective activations\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    data_t = []\n",
        "    # used to iterate through columns\n",
        "    st = 0\n",
        "    # used to iterate through column information\n",
        "    for item in output_info:\n",
        "        # for numeric columns a final tanh activation is applied\n",
        "        if item[1] == 'tanh':\n",
        "            ed = st + item[0]\n",
        "            data_t.append(torch.tanh(data[:, st:ed]))\n",
        "            st = ed\n",
        "        # for one-hot-encoded columns, a final gumbel softmax (https://arxiv.org/pdf/1611.01144.pdf) is used\n",
        "        # to sample discrete categories while still allowing for back propagation\n",
        "        elif item[1] == 'softmax':\n",
        "            ed = st + item[0]\n",
        "            # note that as tau approaches 0, a completely discrete one-hot-vector is obtained\n",
        "            data_t.append(F.gumbel_softmax(data[:, st:ed], tau=0.2))\n",
        "            st = ed\n",
        "\n",
        "    act_data = torch.cat(data_t, dim=1)\n",
        "\n",
        "    return act_data\n",
        "\n",
        "def weights_init(model):\n",
        "\n",
        "    \"\"\"\n",
        "    This function initializes the learnable parameters of the convolutional and batch norm layers\n",
        "\n",
        "    Inputs:\n",
        "    1) model->  network for which the parameters need to be initialized\n",
        "\n",
        "    Outputs:\n",
        "    1) network with corresponding weights initialized using the normal distribution\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    classname = model.__class__.__name__\n",
        "\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.normal_(model.weight.data, 0.0, 0.02)\n",
        "\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal_(model.weight.data, 1.0, 0.02)\n",
        "        init.constant_(model.bias.data, 0)\n",
        "\n",
        "class CTABGANSynthesizer:\n",
        "\n",
        "    \"\"\"\n",
        "    This class represents the main model used for training the model and generating synthetic data\n",
        "\n",
        "\n",
        "    Variables:\n",
        "    1) random_dim -> size of the noise vector fed to the generator\n",
        "    2) class_dim -> tuple containing dimensionality of hidden layers for the classifier network\n",
        "    3) num_channels -> no. of channels for deciding respective hidden layers of discriminator and generator networks\n",
        "    4) dside -> height/width of the input data fed to discriminator network\n",
        "    5) gside -> height/width of the input data generated by the generator network\n",
        "    6) l2scale -> parameter to decide strength of regularization of the network based on constraining l2 norm of weights\n",
        "    7) batch_size -> no. of records to be processed in each mini-batch of training\n",
        "    8) epochs -> no. of epochs to train the model\n",
        "    9) device -> type of device to be used for training (i.e., gpu/cpu)\n",
        "    10) generator -> generator network from which data can be generated after training the model\n",
        "\n",
        "    Methods:\n",
        "    1) __init__() -> initializes the model with user specified parameters\n",
        "    2) fit() -> takes the pre-processed training data and associated parameters as input to fit the CTABGANSynthesizer model\n",
        "    3) sample() -> takes as input the no. of data rows to be generated and synthesizes the corresponding no. of data rows\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 class_dim=(256, 256, 256, 256),\n",
        "                 random_dim=100,\n",
        "                 num_channels=64,\n",
        "                 l2scale=1e-5,\n",
        "                 batch_size=500,\n",
        "                 epochs=1):\n",
        "\n",
        "        self.random_dim = random_dim\n",
        "        self.class_dim = class_dim\n",
        "        self.num_channels = num_channels\n",
        "        self.dside = None\n",
        "        self.gside = None\n",
        "        self.l2scale = l2scale\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.generator = None\n",
        "\n",
        "    def fit(self, train_data=pd.DataFrame, categorical=[], mixed={}, type={}):\n",
        "\n",
        "        # obtaining the column index of the target column used for ML tasks\n",
        "        problem_type = None\n",
        "        target_index = None\n",
        "\n",
        "        if type:\n",
        "            problem_type = list(type.keys())[0]\n",
        "            if problem_type:\n",
        "                target_index = train_data.columns.get_loc(type[problem_type])\n",
        "\n",
        "        # transforming pre-processed training data according to different data types\n",
        "        # i.e., mode specific normalisation for numeric and mixed columns and one-hot-encoding for categorical columns\n",
        "        self.transformer = DataTransformer(train_data=train_data, categorical_list=categorical, mixed_dict=mixed)\n",
        "        self.transformer.fit()\n",
        "        train_data = self.transformer.transform(train_data.values)\n",
        "        # storing column size of the transformed training data\n",
        "        data_dim = self.transformer.output_dim\n",
        "\n",
        "        # initializing the sampler object to execute training-by-sampling\n",
        "        data_sampler = Sampler(train_data, self.transformer.output_info)\n",
        "        # initializing the condvec object to sample conditional vectors during training\n",
        "        self.cond_generator = Condvec(train_data, self.transformer.output_info)\n",
        "\n",
        "        # obtaining the desired height/width for converting tabular data records to square images for feeding it to discriminator network\n",
        "        sides = [4, 8, 16, 24, 32]\n",
        "        # the discriminator takes the transformed training data concatenated by the corresponding conditional vectors as input\n",
        "        col_size_d = data_dim + self.cond_generator.n_opt\n",
        "        for i in sides:\n",
        "            if i * i >= col_size_d:\n",
        "                self.dside = i\n",
        "                break\n",
        "\n",
        "        # obtaining the desired height/width for generating square images from the generator network that can be converted back to tabular domain\n",
        "        sides = [4, 8, 16, 24, 32]\n",
        "        col_size_g = data_dim\n",
        "        for i in sides:\n",
        "            if i * i >= col_size_g:\n",
        "                self.gside = i\n",
        "                break\n",
        "\n",
        "        # constructing the generator and discriminator networks\n",
        "        layers_G = determine_layers_gen(self.gside, self.random_dim+self.cond_generator.n_opt, self.num_channels)\n",
        "        layers_D = determine_layers_disc(self.dside, self.num_channels)\n",
        "        self.generator = Generator(layers_G).to(self.device)\n",
        "        discriminator = Discriminator(layers_D).to(self.device)\n",
        "\n",
        "        # assigning the respective optimizers for the generator and discriminator networks\n",
        "        optimizer_params = dict(lr=2e-4, betas=(0.5, 0.9), eps=1e-3, weight_decay=self.l2scale)\n",
        "        optimizerG = Adam(self.generator.parameters(), **optimizer_params)\n",
        "        optimizerD = Adam(discriminator.parameters(), **optimizer_params)\n",
        "\n",
        "\n",
        "        st_ed = None\n",
        "        classifier=None\n",
        "        optimizerC= None\n",
        "        if target_index != None:\n",
        "            # obtaining the one-hot-encoding starting and ending positions of the target column in the transformed data\n",
        "            st_ed= get_st_ed(target_index,self.transformer.output_info)\n",
        "            # configuring the classifier network and it's optimizer accordingly\n",
        "            classifier = Classifier(data_dim,self.class_dim,st_ed).to(self.device)\n",
        "            optimizerC = optim.Adam(classifier.parameters(),**optimizer_params)\n",
        "\n",
        "        # initializing learnable parameters of the discrimnator and generator networks\n",
        "        self.generator.apply(weights_init)\n",
        "        discriminator.apply(weights_init)\n",
        "\n",
        "        # initializing the image transformer objects for the generator and discriminator networks for transitioning between image and tabular domain\n",
        "        self.Gtransformer = ImageTransformer(self.gside)\n",
        "        self.Dtransformer = ImageTransformer(self.dside)\n",
        "\n",
        "        # initiating the training by computing the number of iterations per epoch\n",
        "        steps_per_epoch = max(1, len(train_data) // self.batch_size)\n",
        "        for i in tqdm(range(self.epochs)):\n",
        "            for _ in range(steps_per_epoch):\n",
        "\n",
        "                # sampling noise vectors using a standard normal distribution\n",
        "                noisez = torch.randn(self.batch_size, self.random_dim, device=self.device)\n",
        "                # sampling conditional vectors\n",
        "                condvec = self.cond_generator.sample_train(self.batch_size)\n",
        "                c, m, col, opt = condvec\n",
        "                c = torch.from_numpy(c).to(self.device)\n",
        "                m = torch.from_numpy(m).to(self.device)\n",
        "                # concatenating conditional vectors and converting resulting noise vectors into the image domain to be fed to the generator as input\n",
        "                noisez = torch.cat([noisez, c], dim=1)\n",
        "                noisez =  noisez.view(self.batch_size,self.random_dim+self.cond_generator.n_opt,1,1)\n",
        "\n",
        "                # sampling real data according to the conditional vectors and shuffling it before feeding to discriminator to isolate conditional loss on generator\n",
        "                perm = np.arange(self.batch_size)\n",
        "                np.random.shuffle(perm)\n",
        "                real = data_sampler.sample(self.batch_size, col[perm], opt[perm])\n",
        "                real = torch.from_numpy(real.astype('float32')).to(self.device)\n",
        "\n",
        "                # storing shuffled ordering of the conditional vectors\n",
        "                c_perm = c[perm]\n",
        "                # generating synthetic data as an image\n",
        "                fake = self.generator(noisez)\n",
        "                # converting it into the tabular domain as per format of the trasformed training data\n",
        "                faket = self.Gtransformer.inverse_transform(fake)\n",
        "                # applying final activation on the generated data (i.e., tanh for numeric and gumbel-softmax for categorical)\n",
        "                fakeact = apply_activate(faket, self.transformer.output_info)\n",
        "\n",
        "                # the generated data is then concatenated with the corresponding condition vectors\n",
        "                fake_cat = torch.cat([fakeact, c], dim=1)\n",
        "                # the real data is also similarly concatenated with corresponding conditional vectors\n",
        "                real_cat = torch.cat([real, c_perm], dim=1)\n",
        "\n",
        "                # transforming the real and synthetic data into the image domain for feeding it to the discriminator\n",
        "                real_cat_d = self.Dtransformer.transform(real_cat)\n",
        "                fake_cat_d = self.Dtransformer.transform(fake_cat)\n",
        "\n",
        "                # executing the gradient update step for the discriminator\n",
        "                optimizerD.zero_grad()\n",
        "                # computing the probability of the discriminator to correctly classify real samples hence y_real should ideally be close to 1\n",
        "                y_real,_ = discriminator(real_cat_d)\n",
        "                # computing the probability of the discriminator to correctly classify fake samples hence y_fake should ideally be close to 0\n",
        "                y_fake,_ = discriminator(fake_cat_d)\n",
        "                # computing the loss to essentially maximize the log likelihood of correctly classifiying real and fake samples as log(D(x))+log(1−D(G(z)))\n",
        "                # or equivalently minimizing the negative of log(D(x))+log(1−D(G(z))) as done below\n",
        "                loss_d = (-(torch.log(y_real + 1e-4).mean()) - (torch.log(1. - y_fake + 1e-4).mean()))\n",
        "                # accumulating gradients based on the loss\n",
        "                loss_d.backward()\n",
        "                # computing the backward step to update weights of the discriminator\n",
        "                optimizerD.step()\n",
        "\n",
        "                # similarly sample noise vectors and conditional vectors\n",
        "                noisez = torch.randn(self.batch_size, self.random_dim, device=self.device)\n",
        "                condvec = self.cond_generator.sample_train(self.batch_size)\n",
        "                c, m, col, opt = condvec\n",
        "                c = torch.from_numpy(c).to(self.device)\n",
        "                m = torch.from_numpy(m).to(self.device)\n",
        "                noisez = torch.cat([noisez, c], dim=1)\n",
        "                noisez =  noisez.view(self.batch_size,self.random_dim+self.cond_generator.n_opt,1,1)\n",
        "\n",
        "                # executing the gradient update step for the generator\n",
        "                optimizerG.zero_grad()\n",
        "\n",
        "                # similarly generating synthetic data and applying final activation\n",
        "                fake = self.generator(noisez)\n",
        "                faket = self.Gtransformer.inverse_transform(fake)\n",
        "                fakeact = apply_activate(faket, self.transformer.output_info)\n",
        "                # concatenating conditional vectors and converting it to the image domain to be fed to the discriminator\n",
        "                fake_cat = torch.cat([fakeact, c], dim=1)\n",
        "                fake_cat = self.Dtransformer.transform(fake_cat)\n",
        "\n",
        "                # computing the probability of the discriminator classifiying fake samples as real\n",
        "                # along with feature representaions of fake data resulting from the penultimate layer\n",
        "                y_fake,info_fake = discriminator(fake_cat)\n",
        "                # extracting feature representation of real data from the penultimate layer of the discriminator\n",
        "                _,info_real = discriminator(real_cat_d)\n",
        "                # computing the conditional loss to ensure the generator generates data records with the chosen category as per the conditional vector\n",
        "                cross_entropy = cond_loss(faket, self.transformer.output_info, c, m)\n",
        "\n",
        "                # computing the loss to train the generator where we want y_fake to be close to 1 to fool the discriminator\n",
        "                # and cross_entropy to be close to 0 to ensure generator's output matches the conditional vector\n",
        "                g = -(torch.log(y_fake + 1e-4).mean()) + cross_entropy\n",
        "                # in order to backprop the gradient of separate losses w.r.t to the learnable weight of the network independently\n",
        "                # we may use retain_graph=True in backward() method in the first back-propagated loss\n",
        "                # to maintain the computation graph to execute the second backward pass efficiently\n",
        "                g.backward(retain_graph=True)\n",
        "                # computing the information loss by comparing means and stds of real/fake feature representations extracted from discriminator's penultimate layer\n",
        "                loss_mean = torch.norm(torch.mean(info_fake.view(self.batch_size,-1), dim=0) - torch.mean(info_real.view(self.batch_size,-1), dim=0), 1)\n",
        "                loss_std = torch.norm(torch.std(info_fake.view(self.batch_size,-1), dim=0) - torch.std(info_real.view(self.batch_size,-1), dim=0), 1)\n",
        "                loss_info = loss_mean + loss_std\n",
        "                # computing the finally accumulated gradients\n",
        "                loss_info.backward()\n",
        "                # executing the backward step to update the weights\n",
        "                optimizerG.step()\n",
        "\n",
        "                # the classifier module is used in case there is a target column associated with ML tasks\n",
        "                if problem_type:\n",
        "\n",
        "                    c_loss = None\n",
        "                    # in case of binary classification, the binary cross entropy loss is used\n",
        "                    if (st_ed[1] - st_ed[0])==2:\n",
        "                        c_loss = BCELoss()\n",
        "                    # in case of multi-class classification, the standard cross entropy loss is used\n",
        "                    else: c_loss = CrossEntropyLoss()\n",
        "\n",
        "                    # updating the weights of the classifier\n",
        "                    optimizerC.zero_grad()\n",
        "                    # computing classifier's target column predictions on the real data along with returning corresponding true labels\n",
        "                    real_pre, real_label = classifier(real)\n",
        "                    if (st_ed[1] - st_ed[0])==2:\n",
        "                        real_label = real_label.type_as(real_pre)\n",
        "                    # computing the loss to train the classifier so that it can perform well on the real data\n",
        "                    loss_cc = c_loss(real_pre, real_label)\n",
        "                    loss_cc.backward()\n",
        "                    optimizerC.step()\n",
        "\n",
        "                    # updating the weights of the generator\n",
        "                    optimizerG.zero_grad()\n",
        "                    # generate synthetic data and apply the final activation\n",
        "                    fake = self.generator(noisez)\n",
        "                    faket = self.Gtransformer.inverse_transform(fake)\n",
        "                    fakeact = apply_activate(faket, self.transformer.output_info)\n",
        "                    # computing classifier's target column predictions on the fake data along with returning corresponding true labels\n",
        "                    fake_pre, fake_label = classifier(fakeact)\n",
        "                    if (st_ed[1] - st_ed[0])==2:\n",
        "                        fake_label = fake_label.type_as(fake_pre)\n",
        "                    # computing the loss to train the generator to improve semantic integrity between target column and rest of the data\n",
        "                    loss_cg = c_loss(fake_pre, fake_label)\n",
        "                    loss_cg.backward()\n",
        "                    optimizerG.step()\n",
        "\n",
        "\n",
        "    def sample(self, n):\n",
        "\n",
        "        # turning the generator into inference mode to effectively use running statistics in batch norm layers\n",
        "        self.generator.eval()\n",
        "        # column information associated with the transformer fit to the pre-processed training data\n",
        "        output_info = self.transformer.output_info\n",
        "\n",
        "        # generating synthetic data in batches accordingly to the total no. required\n",
        "        steps = n // self.batch_size + 1\n",
        "        data = []\n",
        "        for _ in range(steps):\n",
        "            # generating synthetic data using sampled noise and conditional vectors\n",
        "            noisez = torch.randn(self.batch_size, self.random_dim, device=self.device)\n",
        "            condvec = self.cond_generator.sample(self.batch_size)\n",
        "            c = condvec\n",
        "            c = torch.from_numpy(c).to(self.device)\n",
        "            noisez = torch.cat([noisez, c], dim=1)\n",
        "            noisez =  noisez.view(self.batch_size,self.random_dim+self.cond_generator.n_opt,1,1)\n",
        "            fake = self.generator(noisez)\n",
        "            faket = self.Gtransformer.inverse_transform(fake)\n",
        "            fakeact = apply_activate(faket,output_info)\n",
        "            data.append(fakeact.detach().cpu().numpy())\n",
        "\n",
        "        data = np.concatenate(data, axis=0)\n",
        "\n",
        "        # applying the inverse transform and returning synthetic data in a similar form as the original pre-processed training data\n",
        "        result = self.transformer.inverse_transform(data)\n",
        "\n",
        "        return result[0:n]\n"
      ],
      "metadata": {
        "id": "vvhI-9ja4OXz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbuT0DTNHR2n",
        "outputId": "9c4464e7-7b66-4b49-d762-b77899c2747d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/CTGAN/train.csv')\n",
        "\n",
        "# Ensure the target column exists\n",
        "target_column = 'Survived'\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['Sex', 'Embarked', 'Pclass']  # Pclass is actually numerical but often treated as categorical\n",
        "numerical_cols = data.columns.difference(categorical_cols + [target_column, 'Name', 'Ticket', 'Cabin'])  # Exclude target column and irrelevant columns\n",
        "\n",
        "# Impute missing values for categorical columns\n",
        "imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
        "data[categorical_cols] = imputer_categorical.fit_transform(data[categorical_cols])\n",
        "\n",
        "# Impute missing values for numerical columns\n",
        "imputer_numerical = SimpleImputer(strategy='mean')\n",
        "data[numerical_cols] = imputer_numerical.fit_transform(data[numerical_cols])\n",
        "\n",
        "# Label encode categorical columns\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Combine numerical and encoded categorical columns\n",
        "data_combined = pd.concat([pd.DataFrame(data[numerical_cols]), data[categorical_cols], data[[target_column]]], axis=1)\n",
        "\n",
        "# Show the first few rows of the processed dataset\n",
        "print(data_combined.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFEr2uvZ4Obx",
        "outputId": "161122ef-09f0-4ae8-d4af-1bcb26dc56ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Age     Fare  Parch  PassengerId  SibSp  Sex  Embarked  Pclass  Survived\n",
            "0  22.0   7.2500    0.0          1.0    1.0    1         2       2         0\n",
            "1  38.0  71.2833    0.0          2.0    1.0    0         0       0         1\n",
            "2  26.0   7.9250    0.0          3.0    0.0    0         2       2         1\n",
            "3  35.0  53.1000    0.0          4.0    1.0    0         2       0         1\n",
            "4  35.0   8.0500    0.0          5.0    0.0    1         2       2         0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synthesizer = CTABGANSynthesizer()\n",
        "\n",
        "# Fit the model on the dataset\n",
        "synthesizer.fit(data_combined)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcuzlndN4K7E",
        "outputId": "4b00b442-0f80-47b8-df68-2fbfa3a1fa73"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.29s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_data = synthesizer.sample(n=1000)"
      ],
      "metadata": {
        "id": "97sHkgPo4LH-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRp63_rgACig",
        "outputId": "fcb5a9eb-ebe5-4979-b128-ecb1545a40cd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_combined.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmSRJN3NAQIn",
        "outputId": "561191d1-cd63-4328-cb97-0cd2133d2cf9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(891, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pplGfW_YAVuP"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}